{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engine performance analysis notebook\n",
    "\n",
    "This notebook ingests the `engine-perf` harness artifact, extracts scheduler/GC telemetry, and derives updated AutoParry\n",
    "parameters for the heavy-load manifests. Run it after `python tests/run_harness.py --suite engine-perf` to compare commits\n",
    "and feed tuned values back into `tests/engine/perf.manifest.lua`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "\n",
    "ARTIFACT_PATH = Path(\"tests/artifacts/engine-perf/engine_perf_metrics.json\")\n",
    "if not ARTIFACT_PATH.exists():\n",
    "    raise FileNotFoundError(\"Run `python tests/run_harness.py --suite engine-perf` to generate engine perf artifacts.\")\n",
    "\n",
    "payload = json.loads(ARTIFACT_PATH.read_text())\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_scheduler_snapshots(metrics_payload: dict[str, object]) -> dict[str, dict[str, float]]:\n",
    "    snapshots: dict[str, dict[str, float]] = {}\n",
    "    for scenario in metrics_payload.get(\"scenarios\", []):\n",
    "        entry_metrics = scenario.get(\"metrics\", {})\n",
    "        perf = entry_metrics.get(\"performance\", {})\n",
    "        scheduler = perf.get(\"scheduler\")\n",
    "        if not scheduler:\n",
    "            continue\n",
    "        snapshots[scenario.get(\"id\", \"unknown\")] = {\n",
    "            \"utilisation\": scheduler.get(\"utilisation\", 0.0) or 0.0,\n",
    "            \"avg_step\": scheduler.get(\"averageStep\", 0.0) or 0.0,\n",
    "            \"events_per_step\": scheduler.get(\"events\", {}).get(\"perStep\", 0.0) or 0.0,\n",
    "            \"queue_depth\": scheduler.get(\"queue\", {}).get(\"averageDepth\", 0.0) or 0.0,\n",
    "        }\n",
    "    return snapshots\n",
    "\n",
    "scheduler_snapshots = collect_scheduler_snapshots(payload)\n",
    "scheduler_snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recommend_config(snapshots: dict[str, dict[str, float]]) -> dict[str, dict[str, float]]:\n",
    "    recommendations: dict[str, dict[str, float]] = {}\n",
    "    global_avg_util = mean((snapshots[k][\"utilisation\"] for k in snapshots if snapshots[k])) if snapshots else 1.0\n",
    "\n",
    "    for scenario_id, stats in snapshots.items():\n",
    "        util = stats.get(\"utilisation\", 1.0) or 1.0\n",
    "        avg_step = stats.get(\"avg_step\", 0.0) or 0.0\n",
    "        queue_depth = stats.get(\"queue_depth\", 0.0) or 0.0\n",
    "\n",
    "        slack = max(0.006, min(0.014, avg_step * 2.0))\n",
    "        cooldown = max(0.08, min(0.14, 0.12 * util / max(0.01, global_avg_util)))\n",
    "        lookahead = max(0.82, min(0.98, 0.9 + (queue_depth * 0.01)))\n",
    "\n",
    "        recommendations[scenario_id] = {\n",
    "            \"pressScheduleSlack\": round(slack, 5),\n",
    "            \"cooldown\": round(cooldown, 5),\n",
    "            \"pressMaxLookahead\": round(lookahead, 5),\n",
    "        }\n",
    "    return recommendations\n",
    "\n",
    "config_updates = recommend_config(scheduler_snapshots)\n",
    "config_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MANIFEST_PATH = Path(\"tests/engine/perf.manifest.lua\")\n",
    "manifest_text = MANIFEST_PATH.read_text()\n",
    "\n",
    "def apply_updates(text: str, scenario_id: str, updates: dict[str, float]) -> str:\n",
    "    for key, value in updates.items():\n",
    "        pattern = rf'(id\\s*=\\s*\"{re.escape(scenario_id)}\".*?{key}\\s*=\\s*)([0-9.]+)'\n",
    "        replacement = rf'\\g<1>{value}'\n",
    "        text, count = re.subn(pattern, replacement, text, count=1, flags=re.S)\n",
    "        if count == 0:\n",
    "            print(f\"[warn] {scenario_id}: unable to update {key}\")\n",
    "    return text\n",
    "\n",
    "updated_text = manifest_text\n",
    "for scenario_id, updates in config_updates.items():\n",
    "    updated_text = apply_updates(updated_text, scenario_id, updates)\n",
    "\n",
    "if updated_text != manifest_text:\n",
    "    MANIFEST_PATH.write_text(updated_text)\n",
    "    print(\"Updated perf manifest with tuned parameters.\")\n",
    "else:\n",
    "    print(\"No manifest changes were applied (keys may already match).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
